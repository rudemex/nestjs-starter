# docker-compose.yml - Only for local debug purposes
# docker-compose up -d elasticsearch kibana jaeger camunda redis mongo mysql postgres pgadmin grafana
services:
  postgres:
    #image: postgres:13
    image: postgres:15
    container_name: local-postgres
    restart: always
    ports:
      - '5432:5432'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      POSTGRES_DB: test_db
      POSTGRES_USER: root
      POSTGRES_PASSWORD: 123456
    #logging:
    #  driver: none
    #volumes:
    #  - ./.postgres_data:/var/lib/postgresql/data

  pgadmin:
    image: dpage/pgadmin4
    container_name: local-pgadmin
    restart: always
    ports:
      - '5050:80'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      PGADMIN_DEFAULT_EMAIL: root@admin.com
      PGADMIN_DEFAULT_PASSWORD: root

  mysql:
    image: mysql:5.7
    #image: mysql:9.3.0
    container_name: local-mysql
    restart: always
    ports:
      - '3306:3306'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      MYSQL_ROOT_PASSWORD: 123456
      MYSQL_DATABASE: test_db
      MYSQL_PASSWORD: 123456
      #MYSQL_USER: root
    #volumes:
    #  - ./.mysql-data/db:/var/lib/mysql

  mongo:
    #image: mongo:5.0
    image: mongo:8.0.6
    container_name: local-mongo
    restart: always
    ports:
      - '27017:27017'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: 123456
      MONGO_INITDB_DATABASE: test_db
    #logging:
    #  driver: none
    #volumes:
    #  - ./.mongo_data:/data/db

  redis:
    #image: redis:6.2-alpine
    image: redis:7.4.2-alpine
    container_name: local-redis
    restart: always
    ports:
      - '6379:6379'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      REDIS_PORT: 6379
      REDIS_PASSWORD: 123456
      REDIS_HOST: cache
      #REDIS_USERNAME: default
    command: ['redis-server', '--appendonly', 'yes', '--requirepass', '123456']
    #logging:
    #  driver: none
    #volumes:
    #  - ./.redis_data:/data

  dynamodb:
    #image: amazon/dynamodb-local:latest
    #image: amazon/dynamodb-local:1.21.0
    image: amazon/dynamodb-local:2.6.0
    container_name: tresdoce-test-utils-global-dynamodb
    restart: always
    ports:
      - '8000:8000'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      AWS_ACCESS_KEY_ID: local
      AWS_SECRET_ACCESS_KEY: local
      AWS_REGION: us-east-1

  authorizer:
    image: lakhansamani/authorizer:1.4.4
    container_name: local-authorizer
    restart: always
    ports:
      - '3001:3001'
    environment:
      ENV: 'development'
      ADMIN_SECRET: 'admin'
      PORT: 3001
      DATABASE_TYPE: postgres
      DATABASE_URL: postgres://root:123456@postgres:5432/test_db
      REDIS_URL: redis://:123456@redis:6379
      CUSTOM_ACCESS_TOKEN_SCRIPT: function(user,tokenPayload){var data=tokenPayload;data.user=user;return data;}
      DISABLE_PLAYGROUND: false
      DISABLE_EMAIL_VERIFICATION: false
      DISABLE_MAGIC_LINK_LOGIN: true
      ROLES: user,admin,super_admin
      DEFAULT_ROLES: user
      PROTECTED_ROLES: admin,super_admin
      SMTP_HOST:
      SMTP_PORT: 465
      SMTP_USERNAME:
      SMTP_PASSWORD:
      SENDER_EMAIL: onboarding@resend.dev
      SENDER_NAME: Test Authorizer (Resend)
    depends_on:
      - postgres
      - redis

  camunda:
    image: camunda/camunda-bpm-platform:run-latest
    container_name: local-camunda
    restart: always
    ports:
      - '8443:8080'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'

  elasticsearch:
    #image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    image: docker.elastic.co/elasticsearch/elasticsearch:8.18.0
    container_name: local-elasticsearch
    restart: always
    ports:
      - '9200:9200'
    environment:
      - TZ=UTC
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.security.enabled=false
      - ELASTIC_PASSWORD=elastic
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    cap_add:
      - IPC_LOCK
    #volumes:
    #  - ./.elasticsearch-data:/usr/share/elasticsearch/data

  kibana:
    #image: docker.elastic.co/kibana/kibana:8.13.4
    image: docker.elastic.co/kibana/kibana:8.18.0
    container_name: local-kibana
    restart: always
    ports:
      - '5601:5601'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      SERVER_NAME: kibana
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    depends_on:
      - elasticsearch

  jaeger:
    container_name: local-jaeger
    image: jaegertracing/all-in-one:latest
    restart: always
    ports:
      - '6831:6831/udp'
      - '6832:6832/udp'
      - '5778:5778'
      - '16686:16686'
      - '4317:4317'
      - '4318:4318'
      - '14250:14250'
      - '14268:14268'
      - '14269:14269'
      - '9411:9411'
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      COLLECTOR_OTLP_ENABLED: 'true'
      #LOG_LEVEL: debug

  grafana:
    container_name: local-grafana
    image: grafana/grafana:latest
    restart: always
    ports:
      - '3000:3000'
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

  zookeeper:
    container_name: local-zookeeper
    #image: confluentinc/cp-zookeeper:5.5.3
    image: confluentinc/cp-zookeeper:7.8.0
    restart: always
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - '2181:2181'

  kafka:
    container_name: local-kafka
    #image: confluentinc/cp-kafka:5.5.3
    image: confluentinc/cp-kafka:7.8.0
    restart: always
    environment:
      TZ: 'America/Argentina/Buenos_Aires'
      KAFKA_BROKER_ID: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_CREATE_TOPICS: 'test.topic:1:1'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://docker:9092
      KAFKA_LISTENERS: PLAINTEXT://:29092,PLAINTEXT_HOST://:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9997
      KAFKA_JMX_HOSTNAME: kafka
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG4J_LOGGERS: >-
        kafka.controller=INFO,
        kafka.producer.async.DefaultEventHandler=INFO,
        state.change.logger=INFO
    ports:
      - '9092:9092'
      - '9997:9997'
    depends_on:
      - zookeeper
    command: >
      bash -c "
        echo 'Making magic 2.0'
        echo 'echo \"Waiting for Kafka to start listening on localhost...\";' > startup.sh
        echo 'start_timeout_exceeded=false' >> startup.sh
        echo 'count=0' >> startup.sh
        echo 'while ! nc -z kafka 29092; do' >> startup.sh
        echo '  echo \"Waiting for Kafka to start listening on localhost...\"' >> startup.sh
        echo '  sleep 1' >> startup.sh
        echo '  ((count+=1))' >> startup.sh
        echo '  if [ $$count -eq 30 ]; then' >> startup.sh
        echo '    start_timeout_exceeded=true' >> startup.sh
        echo '    break' >> startup.sh
        echo '  fi' >> startup.sh
        echo 'done' >> startup.sh
        echo 'if [ \"$$start_timeout_exceeded\" = true ]; then' >> startup.sh
        echo '  echo \"Kafka did not start within 30 seconds. Exiting...\"' >> startup.sh
        echo '  exit 1' >> startup.sh
        echo 'fi' >> startup.sh
        echo 'echo \"Kafka is now listening on localhost:9092\"' >> startup.sh

        echo 'kafka-topics --bootstrap-server kafka:29092 --list' >> startup.sh
        echo 'echo \"Kafka started\";' >> startup.sh
        #echo 'kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test.topic' >> startup.sh
        #echo 'kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test.topic.a' >> startup.sh
        #echo 'kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test.topic.b' >> startup.sh
        #echo 'kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test.topic.c' >> startup.sh
        # for kafka 7.8.0
        echo 'kafka-topics --create --if-not-exists --bootstrap-server docker:9092 --replication-factor 1 --partitions 1 --topic test.topic' >> startup.sh
        echo 'kafka-topics --create --if-not-exists --bootstrap-server docker:9092 --replication-factor 1 --partitions 1 --topic test.topic.a' >> startup.sh
        echo 'kafka-topics --create --if-not-exists --bootstrap-server docker:9092 --replication-factor 1 --partitions 1 --topic test.topic.b' >> startup.sh
        echo 'kafka-topics --create --if-not-exists --bootstrap-server docker:9092 --replication-factor 1 --partitions 1 --topic test.topic.c' >> startup.sh
        
        echo 'echo \"Successfully created the following topics:\"' >> startup.sh
        echo 'kafka-topics --bootstrap-server kafka:29092 --list' >> startup.sh
        chmod a+x startup.sh
        cat startup.sh
        ./startup.sh &
        exec /etc/confluent/docker/run
      "

  schema-registry:
    container_name: local-kafka-schema-registry
    #image: confluentinc/cp-schema-registry:5.5.3
    image: confluentinc/cp-schema-registry:7.8.0
    restart: always
    depends_on:
      - kafka
    ports:
      - '8085:8085'
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8085
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://docker:9092

  kafka_ui:
    container_name: local-kafka-ui
    image: provectuslabs/kafka-ui:latest
    restart: always
    environment:
      KAFKA_CLUSTERS_0_ZOOKEEPER: 'zookeeper:2181'
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_METRICS_PORT: 9997
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8085
      DYNAMIC_CONFIG_ENABLED: 'true'
    ports:
      - '8282:8080'
    depends_on:
      #- kafka
      - schema-registry

  keycloak:
    #image: quay.io/keycloak/keycloak:25.0.2
    image: quay.io/keycloak/keycloak:26.3.1
    container_name: local-keycloak
    restart: always
    ports:
      - '8443:8443'
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres:5432/test_db
      KC_DB_USERNAME: root
      KC_DB_PASSWORD: 123456
      KC_HOSTNAME: localhost
      KC_HOSTNAME_PORT: 8443
      KC_HOSTNAME_STRICT_BACKCHANNEL: true
      KC_HOSTNAME_STRICT: false
      KC_HOSTNAME_STRICT_HTTPS: false
      KC_LOG_LEVEL: info
      KC_METRICS_ENABLED: true
      KC_HEALTH_ENABLED: true
      KC_BOOTSTRAP_ADMIN_USERNAME: admin
      KC_BOOTSTRAP_ADMIN_PASSWORD: admin
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8443/health/ready']
      interval: 15s
      timeout: 2s
      retries: 15
    command: start-dev --http-port=8443
    depends_on:
      - postgres

  sqs-localstack:
    image: localstack/localstack
    container_name: local-sqs-localstack
    restart: always
    ports:
      - '4566:4566'
    environment:
      SERVICES: sqs
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
      AWS_DEFAULT_REGION: us-east-1
      DEBUG: 0

  aws-cli-sqs:
    image: amazon/aws-cli
    container_name: local-aws-cli-sqs
    depends_on:
      - sqs-localstack
    environment:
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
      AWS_DEFAULT_REGION: us-east-1
    entrypoint: /bin/sh -c
    command: >
      "
        echo 'Waiting for SQS to be available...';
        until curl -s http://sqs-localstack:4566/_localstack/health | grep '\"sqs\": \"available\"'; do
          echo 'Waiting for SQS to be ready...';
          sleep 2;
        done;
        echo 'Creating queues...';
        aws sqs create-queue --endpoint-url=http://sqs-localstack:4566 --queue-name test-queue-1;
        aws sqs create-queue --endpoint-url=http://sqs-localstack:4566 --queue-name test-queue-2;
        aws sqs create-queue --endpoint-url=http://sqs-localstack:4566 --queue-name test-queue-3;
        aws sqs create-queue --endpoint-url=http://sqs-localstack:4566 --queue-name orders;
        aws sqs create-queue --endpoint-url=http://sqs-localstack:4566 --queue-name notifications;
        echo 'Listing queues...';
        aws sqs list-queues --endpoint-url=http://sqs-localstack:4566;
      "

  rabbitmq:
    image: rabbitmq:3-management
    container_name: local-rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: rabbitmq
      RABBITMQ_DEFAULT_PASS: rabbitmq
      RABBITMQ_DEFAULT_VHOST: /
    ports:
      - '5672:5672'
      - '15672:15672'
    #volumes:
    #- rabbitmq_data:/var/lib/rabbitmq
